{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5567d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16dfa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# Model 1\n",
    "#model_checkpoint = \"distilbert-base-uncased\" \n",
    "# Model 2\n",
    "model_checkpoint = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef011b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small, well-known dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")  # Use subset for speed\n",
    "texts, labels = list(dataset['text']), list(dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ada5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "X = tokenized['input_ids'].numpy()\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9b2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Trainer API for simplicity\n",
    "def run_experiment(random_seed, model_checkpoint):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    # Split data\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(y)), test_size=0.3, random_state=random_seed)\n",
    "    # Build datasets\n",
    "    class MyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, idxs):\n",
    "            self.encodings = {k: v[idxs] for k,v in tokenized.items()}\n",
    "            self.labels = torch.tensor(y[idxs])\n",
    "        def __getitem__(self, idx):\n",
    "            return {**{k: v[idx] for k, v in self.encodings.items()}, 'labels': self.labels[idx]}\n",
    "        def __len__(self): return len(self.labels)\n",
    "    train_dataset = MyDataset(train_idx)\n",
    "    test_dataset = MyDataset(test_idx)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=4)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        seed=random_seed,\n",
    "        save_strategy=\"no\",\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result['eval_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbb779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1967, 'grad_norm': 4.941572189331055, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 0.9378, 'grad_norm': 3.9741854667663574, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.7898, 'grad_norm': 6.888270378112793, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.682, 'grad_norm': 9.499796867370605, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.5578, 'grad_norm': 5.945507049560547, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.5687, 'grad_norm': 4.156579971313477, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.6301, 'grad_norm': 10.236400604248047, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.5132, 'grad_norm': 10.129493713378906, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 14.722, 'train_samples_per_second': 47.548, 'train_steps_per_second': 5.977, 'train_loss': 0.7197933251207526, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5000646710395813, 'eval_accuracy': 0.85, 'eval_runtime': 1.4903, 'eval_samples_per_second': 201.305, 'eval_steps_per_second': 25.499, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.358, 'grad_norm': 4.945007801055908, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.0902, 'grad_norm': 4.470273017883301, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9426, 'grad_norm': 4.89274787902832, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.6902, 'grad_norm': 4.711475372314453, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.6907, 'grad_norm': 2.9768786430358887, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.5443, 'grad_norm': 7.343725204467773, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.6351, 'grad_norm': 6.500596523284912, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4502, 'grad_norm': 2.452087640762329, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.1802, 'train_samples_per_second': 62.611, 'train_steps_per_second': 7.871, 'train_loss': 0.7815964872186835, 'epoch': 1.0}\n",
      "{'eval_loss': 0.48394128680229187, 'eval_accuracy': 0.8466666666666667, 'eval_runtime': 1.2916, 'eval_samples_per_second': 232.271, 'eval_steps_per_second': 29.421, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2799, 'grad_norm': 3.2554075717926025, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.0429, 'grad_norm': 4.115146636962891, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.8543, 'grad_norm': 5.335216999053955, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.8608, 'grad_norm': 8.963418960571289, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.7155, 'grad_norm': 2.959280490875244, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.485, 'grad_norm': 3.0826690196990967, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.4521, 'grad_norm': 5.237246036529541, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4023, 'grad_norm': 7.694278240203857, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.2058, 'train_samples_per_second': 62.468, 'train_steps_per_second': 7.853, 'train_loss': 0.7401921315626665, 'epoch': 1.0}\n",
      "{'eval_loss': 0.45167168974876404, 'eval_accuracy': 0.86, 'eval_runtime': 1.3084, 'eval_samples_per_second': 229.284, 'eval_steps_per_second': 29.043, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2129, 'grad_norm': 3.7248950004577637, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.164, 'grad_norm': 5.067765235900879, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9261, 'grad_norm': 4.0508928298950195, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.8348, 'grad_norm': 4.49784517288208, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.6858, 'grad_norm': 6.085018634796143, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.6446, 'grad_norm': 6.382251262664795, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.6099, 'grad_norm': 3.59616756439209, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.5642, 'grad_norm': 3.860992670059204, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.2339, 'train_samples_per_second': 62.311, 'train_steps_per_second': 7.833, 'train_loss': 0.8028793985193426, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5196817517280579, 'eval_accuracy': 0.8433333333333334, 'eval_runtime': 1.3802, 'eval_samples_per_second': 217.363, 'eval_steps_per_second': 27.533, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3032, 'grad_norm': 2.2060065269470215, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.0539, 'grad_norm': 5.4949517250061035, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9337, 'grad_norm': 6.369175434112549, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.6952, 'grad_norm': 2.7352945804595947, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.691, 'grad_norm': 11.469582557678223, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.6066, 'grad_norm': 13.8048095703125, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.589, 'grad_norm': 3.1733040809631348, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4133, 'grad_norm': 5.282905101776123, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.2305, 'train_samples_per_second': 62.33, 'train_steps_per_second': 7.836, 'train_loss': 0.7656606598333879, 'epoch': 1.0}\n",
      "{'eval_loss': 0.49822092056274414, 'eval_accuracy': 0.83, 'eval_runtime': 1.3106, 'eval_samples_per_second': 228.897, 'eval_steps_per_second': 28.994, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2939, 'grad_norm': 2.8107080459594727, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.0517, 'grad_norm': 3.518787145614624, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.7899, 'grad_norm': 6.314426898956299, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.7204, 'grad_norm': 5.498767375946045, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.6206, 'grad_norm': 5.85137939453125, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.4843, 'grad_norm': 1.8235493898391724, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.5814, 'grad_norm': 11.651384353637695, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.552, 'grad_norm': 3.542625665664673, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.1576, 'train_samples_per_second': 62.737, 'train_steps_per_second': 7.887, 'train_loss': 0.7354344590143724, 'epoch': 1.0}\n",
      "{'eval_loss': 0.4392489492893219, 'eval_accuracy': 0.8633333333333333, 'eval_runtime': 1.3056, 'eval_samples_per_second': 229.776, 'eval_steps_per_second': 29.105, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2719, 'grad_norm': 2.980914831161499, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.0443, 'grad_norm': 4.8156256675720215, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.8473, 'grad_norm': 4.634191513061523, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.6507, 'grad_norm': 4.794312477111816, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.615, 'grad_norm': 3.5180466175079346, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.5884, 'grad_norm': 8.332164764404297, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.5751, 'grad_norm': 11.510383605957031, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.5901, 'grad_norm': 3.529625654220581, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.135, 'train_samples_per_second': 62.865, 'train_steps_per_second': 7.903, 'train_loss': 0.7433884794061835, 'epoch': 1.0}\n",
      "{'eval_loss': 0.45954594016075134, 'eval_accuracy': 0.8766666666666667, 'eval_runtime': 1.3026, 'eval_samples_per_second': 230.303, 'eval_steps_per_second': 29.172, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3185, 'grad_norm': 3.287883758544922, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.0719, 'grad_norm': 3.392120599746704, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.8473, 'grad_norm': 5.117605686187744, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.7289, 'grad_norm': 8.613532066345215, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.7253, 'grad_norm': 3.647578239440918, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.7519, 'grad_norm': 5.809255123138428, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.4676, 'grad_norm': 9.757250785827637, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4899, 'grad_norm': 4.472189426422119, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 11.0907, 'train_samples_per_second': 63.116, 'train_steps_per_second': 7.935, 'train_loss': 0.7792176062410529, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5496718287467957, 'eval_accuracy': 0.8133333333333334, 'eval_runtime': 1.3006, 'eval_samples_per_second': 230.667, 'eval_steps_per_second': 29.218, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Run for Model 1 - distilbert-base-uncased\n",
    "\n",
    "# Run multiple times to estimate accuracy and its confidence interval\n",
    "results = []\n",
    "for seed in range(8):\n",
    "    acc = run_experiment(random_seed=seed, model_checkpoint=model_checkpoint)\n",
    "    results.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: distilbert-base-uncased\n",
      "Accuracies: [0.85  0.847 0.86  0.843 0.83  0.863 0.877 0.813]\n",
      "Mean=0.848, 95% CI=(0.834, 0.862)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confidence interval on Model 1 results\n",
    "def compute_ci(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    std_err = np.std(data, ddof=1)/np.sqrt(len(data))\n",
    "    ci = std_err * 1.96  # For 95%\n",
    "    return (mean - ci, mean, mean + ci)\n",
    "\n",
    "ci = compute_ci(results)\n",
    "print(f\"\\nModel: {model_checkpoint}\")\n",
    "print(f\"Accuracies: {np.round(results, 3)}\")\n",
    "print(f\"Mean={ci[1]:.3f}, 95% CI=({ci[0]:.3f}, {ci[2]:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b202a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.264, 'grad_norm': 21.015626907348633, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.1941, 'grad_norm': 15.182701110839844, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9254, 'grad_norm': 23.20159912109375, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.7785, 'grad_norm': 15.699048042297363, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.6744, 'grad_norm': 24.603622436523438, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.705, 'grad_norm': 21.13383674621582, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.6585, 'grad_norm': 34.009437561035156, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4859, 'grad_norm': 58.19181823730469, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.6827, 'train_samples_per_second': 32.284, 'train_steps_per_second': 4.059, 'train_loss': 0.8195769515904513, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5213119387626648, 'eval_accuracy': 0.8366666666666667, 'eval_runtime': 3.2362, 'eval_samples_per_second': 92.703, 'eval_steps_per_second': 11.742, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3173, 'grad_norm': 23.26309585571289, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.1604, 'grad_norm': 17.835969924926758, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 1.0021, 'grad_norm': 47.85809326171875, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.7271, 'grad_norm': 26.501739501953125, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.7673, 'grad_norm': 9.426093101501465, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.6954, 'grad_norm': 19.708019256591797, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.726, 'grad_norm': 28.26484489440918, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4878, 'grad_norm': 9.90575885772705, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.2416, 'train_samples_per_second': 32.954, 'train_steps_per_second': 4.143, 'train_loss': 0.8587819771333174, 'epoch': 1.0}\n",
      "{'eval_loss': 0.6076040863990784, 'eval_accuracy': 0.7966666666666666, 'eval_runtime': 3.1597, 'eval_samples_per_second': 94.946, 'eval_steps_per_second': 12.026, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3699, 'grad_norm': 12.71481990814209, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.2555, 'grad_norm': 10.566231727600098, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 1.0062, 'grad_norm': 11.591489791870117, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.8186, 'grad_norm': 18.825834274291992, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.7294, 'grad_norm': 11.901957511901855, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.5241, 'grad_norm': 3.7994890213012695, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.6169, 'grad_norm': 10.25279712677002, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.4181, 'grad_norm': 60.55865478515625, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.2533, 'train_samples_per_second': 32.936, 'train_steps_per_second': 4.141, 'train_loss': 0.8183919245546515, 'epoch': 1.0}\n",
      "{'eval_loss': 0.507323682308197, 'eval_accuracy': 0.85, 'eval_runtime': 3.1583, 'eval_samples_per_second': 94.987, 'eval_steps_per_second': 12.032, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1992, 'grad_norm': 10.974956512451172, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.1628, 'grad_norm': 54.9497184753418, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9926, 'grad_norm': 25.61110496520996, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.8278, 'grad_norm': 97.79493713378906, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.85, 'grad_norm': 11.796147346496582, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.7473, 'grad_norm': 19.199811935424805, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.7059, 'grad_norm': 6.365169048309326, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.6117, 'grad_norm': 19.67987823486328, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.2497, 'train_samples_per_second': 32.942, 'train_steps_per_second': 4.141, 'train_loss': 0.8628164258870211, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5714531540870667, 'eval_accuracy': 0.8433333333333334, 'eval_runtime': 3.1442, 'eval_samples_per_second': 95.413, 'eval_steps_per_second': 12.086, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3418, 'grad_norm': 21.000940322875977, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 0.9672, 'grad_norm': 29.98208236694336, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.811, 'grad_norm': 48.89148712158203, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.9171, 'grad_norm': 14.86322021484375, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.8263, 'grad_norm': 72.09468078613281, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.7495, 'grad_norm': 14.349154472351074, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.7705, 'grad_norm': 27.50349235534668, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.639, 'grad_norm': 17.12796401977539, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.2705, 'train_samples_per_second': 32.909, 'train_steps_per_second': 4.137, 'train_loss': 0.8501344160600142, 'epoch': 1.0}\n",
      "{'eval_loss': 0.6189822554588318, 'eval_accuracy': 0.8133333333333334, 'eval_runtime': 3.1509, 'eval_samples_per_second': 95.21, 'eval_steps_per_second': 12.06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.282, 'grad_norm': 53.42741394042969, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.2383, 'grad_norm': 53.623291015625, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9448, 'grad_norm': 40.14274978637695, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.8599, 'grad_norm': 157.10487365722656, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.805, 'grad_norm': 203.35594177246094, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.7164, 'grad_norm': 29.50766372680664, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.889, 'grad_norm': 127.65093231201172, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.7518, 'grad_norm': 18.75395393371582, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.3625, 'train_samples_per_second': 32.768, 'train_steps_per_second': 4.119, 'train_loss': 0.9110197533260692, 'epoch': 1.0}\n",
      "{'eval_loss': 0.6620550751686096, 'eval_accuracy': 0.78, 'eval_runtime': 3.1369, 'eval_samples_per_second': 95.634, 'eval_steps_per_second': 12.114, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2957, 'grad_norm': 35.07734298706055, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.1279, 'grad_norm': 39.68547439575195, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 0.9183, 'grad_norm': 46.67231750488281, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 0.7643, 'grad_norm': 23.1479434967041, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.6756, 'grad_norm': 17.044095993041992, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.7803, 'grad_norm': 34.34391784667969, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.6844, 'grad_norm': 44.39559555053711, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.6508, 'grad_norm': 20.664199829101562, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.2817, 'train_samples_per_second': 32.892, 'train_steps_per_second': 4.135, 'train_loss': 0.8409872922030363, 'epoch': 1.0}\n",
      "{'eval_loss': 0.5409886837005615, 'eval_accuracy': 0.84, 'eval_runtime': 3.1545, 'eval_samples_per_second': 95.103, 'eval_steps_per_second': 12.046, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/sakthivelthangaraj/learnings/statistics/.statsvenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4055, 'grad_norm': 6.46197509765625, 'learning_rate': 4.488636363636364e-05, 'epoch': 0.11363636363636363}\n",
      "{'loss': 1.294, 'grad_norm': 30.53546142578125, 'learning_rate': 3.9204545454545456e-05, 'epoch': 0.22727272727272727}\n",
      "{'loss': 1.1865, 'grad_norm': 14.483648300170898, 'learning_rate': 3.352272727272727e-05, 'epoch': 0.3409090909090909}\n",
      "{'loss': 1.1072, 'grad_norm': 43.16218566894531, 'learning_rate': 2.784090909090909e-05, 'epoch': 0.45454545454545453}\n",
      "{'loss': 0.9709, 'grad_norm': 32.465057373046875, 'learning_rate': 2.215909090909091e-05, 'epoch': 0.5681818181818182}\n",
      "{'loss': 0.8833, 'grad_norm': 39.17475891113281, 'learning_rate': 1.6477272727272726e-05, 'epoch': 0.6818181818181818}\n",
      "{'loss': 0.7044, 'grad_norm': 19.659196853637695, 'learning_rate': 1.0795454545454547e-05, 'epoch': 0.7954545454545454}\n",
      "{'loss': 0.8648, 'grad_norm': 27.460329055786133, 'learning_rate': 5.113636363636364e-06, 'epoch': 0.9090909090909091}\n",
      "{'train_runtime': 21.293, 'train_samples_per_second': 32.875, 'train_steps_per_second': 4.133, 'train_loss': 1.0232308506965637, 'epoch': 1.0}\n",
      "{'eval_loss': 0.6896917223930359, 'eval_accuracy': 0.7766666666666666, 'eval_runtime': 3.1403, 'eval_samples_per_second': 95.533, 'eval_steps_per_second': 12.101, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Run for Model 2 - albert-base-v2\n",
    "\n",
    "# Run multiple times to estimate accuracy and its confidence interval\n",
    "results = []\n",
    "for seed in range(8):\n",
    "    acc = run_experiment(random_seed=seed, model_checkpoint=model_checkpoint)\n",
    "    results.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f00680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: albert-base-v2\n",
      "Accuracies: [0.837 0.797 0.85  0.843 0.813 0.78  0.84  0.777]\n",
      "Mean=0.817, 95% CI=(0.797, 0.838)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confidence interval on Model 2 results\n",
    "def compute_ci(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    std_err = np.std(data, ddof=1)/np.sqrt(len(data))\n",
    "    ci = std_err * 1.96  # For 95%\n",
    "    return (mean - ci, mean, mean + ci)\n",
    "\n",
    "ci = compute_ci(results)\n",
    "print(f\"\\nModel: {model_checkpoint}\")\n",
    "print(f\"Accuracies: {np.round(results, 3)}\")\n",
    "print(f\"Mean={ci[1]:.3f}, 95% CI=({ci[0]:.3f}, {ci[2]:.3f})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".statsvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
